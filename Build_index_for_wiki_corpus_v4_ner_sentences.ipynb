{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import xapian\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBPATH = \"ner_sents_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DBPATH):\n",
    "    os.mkdir(DBPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ner(s):\n",
    "    #s = re.sub(\"^(The|the|A|a|An|an)\\s\", \"\", s)\n",
    "    #s = re.sub(\"\\s\", \"_\", s)\n",
    "    #s = re.sub(\"-\", \"_\", s)\n",
    "    return s\n",
    "\n",
    "def obtain_entities(v):\n",
    "    ents = {preprocess_ner(item['entity']) for item in v['named_entities']}\n",
    "    ents_roots = {preprocess_ner(item['root']) for item in v['named_entities']}\n",
    "    nps = {preprocess_ner(item['noun_phrase']) for item in v['noun_phrases']}\n",
    "    nps_roots = {preprocess_ner(item['root']) for item in v['noun_phrases']}\n",
    "    entities = ents.union(nps).union(ents_roots).union(nps_roots)\n",
    "    return {item for item in entities if len(item) < 100}\n",
    "\n",
    "def read_ner_data(path):\n",
    "    df = pd.read_json(path, orient=\"split\")\n",
    "    df.index.name = 'page_id'\n",
    "    df['entities'] = df.parsed_text.apply(obtain_entities)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for path in sorted(glob('data/corpus_sentences/*.json')):\n",
    "    if re.match(\".*\\/2.\\d+\\.json\", path):\n",
    "        continue\n",
    "    #print(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    shard = \"\"\n",
    "    m = re.match(\".*\\/(\\d+).json\", path)\n",
    "    if m: shard = m[1]\n",
    "\n",
    "    df = read_ner_data(path)\n",
    "    df.reset_index(inplace=True)\n",
    "    df['id'] = df.page_id + '_' + df.sentence.astype(str)\n",
    "    df.set_index('id', inplace=True)\n",
    "df.loc['Islam_in_Australia_12'].to_frame().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/corpus_sentences/001.json\n",
      "data/corpus_sentences/002.json\n",
      "data/corpus_sentences/003.json\n",
      "data/corpus_sentences/004.json\n",
      "data/corpus_sentences/005.json\n",
      "data/corpus_sentences/006.json\n",
      "data/corpus_sentences/007.json\n",
      "data/corpus_sentences/008.json\n",
      "data/corpus_sentences/009.json\n",
      "data/corpus_sentences/010.json\n",
      "data/corpus_sentences/011.json\n",
      "data/corpus_sentences/012.json\n",
      "data/corpus_sentences/013.json\n",
      "data/corpus_sentences/014.json\n",
      "data/corpus_sentences/015.json\n",
      "data/corpus_sentences/016.json\n",
      "data/corpus_sentences/017.json\n",
      "data/corpus_sentences/018.json\n",
      "data/corpus_sentences/019.json\n",
      "data/corpus_sentences/020.json\n",
      "data/corpus_sentences/021.json\n",
      "data/corpus_sentences/022.json\n",
      "data/corpus_sentences/023.json\n",
      "data/corpus_sentences/024.json\n",
      "data/corpus_sentences/025.json\n",
      "data/corpus_sentences/026.json\n",
      "data/corpus_sentences/027.json\n",
      "data/corpus_sentences/028.json\n",
      "data/corpus_sentences/029.json\n",
      "data/corpus_sentences/030.json\n",
      "data/corpus_sentences/031.json\n",
      "data/corpus_sentences/032.json\n",
      "data/corpus_sentences/033.json\n",
      "data/corpus_sentences/034.json\n",
      "data/corpus_sentences/035.json\n",
      "data/corpus_sentences/036.json\n",
      "data/corpus_sentences/037.json\n",
      "data/corpus_sentences/038.json\n",
      "data/corpus_sentences/039.json\n",
      "data/corpus_sentences/040.json\n",
      "data/corpus_sentences/041.json\n",
      "data/corpus_sentences/042.json\n",
      "data/corpus_sentences/043.json\n",
      "data/corpus_sentences/044.json\n",
      "data/corpus_sentences/045.json\n",
      "data/corpus_sentences/046.json\n",
      "data/corpus_sentences/047.json\n",
      "data/corpus_sentences/048.json\n",
      "data/corpus_sentences/049.json\n",
      "data/corpus_sentences/050.json\n",
      "data/corpus_sentences/051.json\n",
      "data/corpus_sentences/052.json\n",
      "data/corpus_sentences/053.json\n",
      "data/corpus_sentences/054.json\n",
      "data/corpus_sentences/055.json\n",
      "data/corpus_sentences/056.json\n",
      "data/corpus_sentences/057.json\n",
      "data/corpus_sentences/058.json\n",
      "data/corpus_sentences/059.json\n",
      "data/corpus_sentences/060.json\n",
      "data/corpus_sentences/061.json\n",
      "data/corpus_sentences/062.json\n",
      "data/corpus_sentences/063.json\n",
      "data/corpus_sentences/064.json\n",
      "data/corpus_sentences/065.json\n",
      "data/corpus_sentences/066.json\n",
      "data/corpus_sentences/067.json\n",
      "data/corpus_sentences/068.json\n",
      "data/corpus_sentences/069.json\n",
      "data/corpus_sentences/070.json\n",
      "data/corpus_sentences/071.json\n",
      "data/corpus_sentences/072.json\n",
      "data/corpus_sentences/073.json\n",
      "data/corpus_sentences/074.json\n",
      "data/corpus_sentences/075.json\n",
      "data/corpus_sentences/076.json\n",
      "data/corpus_sentences/077.json\n",
      "data/corpus_sentences/078.json\n",
      "data/corpus_sentences/079.json\n",
      "data/corpus_sentences/080.json\n",
      "data/corpus_sentences/081.json\n",
      "data/corpus_sentences/082.json\n",
      "data/corpus_sentences/083.json\n",
      "data/corpus_sentences/084.json\n",
      "data/corpus_sentences/085.json\n",
      "data/corpus_sentences/086.json\n",
      "data/corpus_sentences/087.json\n",
      "data/corpus_sentences/088.json\n",
      "data/corpus_sentences/089.json\n",
      "data/corpus_sentences/090.json\n",
      "data/corpus_sentences/091.json\n",
      "data/corpus_sentences/092.json\n",
      "data/corpus_sentences/093.json\n",
      "data/corpus_sentences/094.json\n",
      "data/corpus_sentences/095.json\n",
      "data/corpus_sentences/096.json\n",
      "data/corpus_sentences/097.json\n",
      "data/corpus_sentences/098.json\n",
      "data/corpus_sentences/099.json\n",
      "data/corpus_sentences/100.json\n",
      "data/corpus_sentences/101.json\n",
      "data/corpus_sentences/102.json\n",
      "data/corpus_sentences/103.json\n",
      "data/corpus_sentences/104.json\n",
      "data/corpus_sentences/105.json\n",
      "data/corpus_sentences/106.json\n",
      "data/corpus_sentences/107.json\n",
      "data/corpus_sentences/108.json\n",
      "data/corpus_sentences/109.json\n",
      "data/corpus_sentences/2.001.json\n",
      "data/corpus_sentences/2.002.json\n",
      "data/corpus_sentences/2.003.json\n",
      "data/corpus_sentences/2.004.json\n",
      "data/corpus_sentences/2.005.json\n",
      "data/corpus_sentences/2.006.json\n",
      "data/corpus_sentences/2.007.json\n",
      "data/corpus_sentences/2.008.json\n",
      "data/corpus_sentences/2.009.json\n",
      "data/corpus_sentences/2.010.json\n",
      "data/corpus_sentences/2.011.json\n",
      "data/corpus_sentences/2.012.json\n",
      "data/corpus_sentences/2.013.json\n",
      "data/corpus_sentences/2.014.json\n",
      "data/corpus_sentences/2.015.json\n",
      "data/corpus_sentences/2.016.json\n",
      "data/corpus_sentences/2.017.json\n",
      "data/corpus_sentences/2.018.json\n",
      "data/corpus_sentences/2.019.json\n",
      "data/corpus_sentences/2.020.json\n",
      "data/corpus_sentences/2.021.json\n",
      "data/corpus_sentences/2.022.json\n",
      "data/corpus_sentences/2.023.json\n",
      "data/corpus_sentences/2.024.json\n",
      "data/corpus_sentences/2.025.json\n",
      "data/corpus_sentences/2.026.json\n",
      "data/corpus_sentences/2.027.json\n",
      "data/corpus_sentences/2.028.json\n",
      "data/corpus_sentences/2.029.json\n",
      "data/corpus_sentences/2.030.json\n",
      "data/corpus_sentences/2.031.json\n",
      "data/corpus_sentences/2.032.json\n",
      "data/corpus_sentences/2.033.json\n",
      "data/corpus_sentences/2.034.json\n",
      "data/corpus_sentences/2.035.json\n",
      "data/corpus_sentences/2.036.json\n",
      "data/corpus_sentences/2.037.json\n",
      "data/corpus_sentences/2.038.json\n",
      "data/corpus_sentences/2.039.json\n",
      "data/corpus_sentences/2.040.json\n",
      "data/corpus_sentences/2.041.json\n",
      "data/corpus_sentences/2.042.json\n",
      "data/corpus_sentences/2.043.json\n",
      "data/corpus_sentences/2.044.json\n",
      "data/corpus_sentences/2.045.json\n",
      "data/corpus_sentences/2.046.json\n",
      "data/corpus_sentences/2.047.json\n",
      "data/corpus_sentences/2.048.json\n",
      "data/corpus_sentences/2.049.json\n",
      "data/corpus_sentences/2.050.json\n",
      "data/corpus_sentences/2.051.json\n",
      "data/corpus_sentences/2.052.json\n",
      "data/corpus_sentences/2.053.json\n",
      "data/corpus_sentences/2.054.json\n",
      "data/corpus_sentences/2.055.json\n",
      "data/corpus_sentences/2.056.json\n",
      "data/corpus_sentences/2.057.json\n",
      "data/corpus_sentences/2.058.json\n",
      "data/corpus_sentences/2.059.json\n",
      "data/corpus_sentences/2.060.json\n",
      "data/corpus_sentences/2.061.json\n",
      "data/corpus_sentences/2.062.json\n",
      "data/corpus_sentences/2.063.json\n",
      "data/corpus_sentences/2.064.json\n",
      "data/corpus_sentences/2.065.json\n",
      "data/corpus_sentences/2.066.json\n",
      "data/corpus_sentences/2.067.json\n",
      "data/corpus_sentences/2.068.json\n",
      "data/corpus_sentences/2.069.json\n",
      "data/corpus_sentences/2.070.json\n",
      "data/corpus_sentences/2.071.json\n",
      "data/corpus_sentences/2.072.json\n",
      "data/corpus_sentences/2.073.json\n",
      "data/corpus_sentences/2.074.json\n",
      "data/corpus_sentences/2.075.json\n",
      "data/corpus_sentences/2.076.json\n",
      "data/corpus_sentences/2.077.json\n",
      "data/corpus_sentences/2.078.json\n",
      "data/corpus_sentences/2.079.json\n",
      "data/corpus_sentences/2.080.json\n",
      "data/corpus_sentences/2.081.json\n",
      "data/corpus_sentences/2.082.json\n",
      "data/corpus_sentences/2.083.json\n",
      "data/corpus_sentences/2.084.json\n",
      "data/corpus_sentences/2.085.json\n",
      "data/corpus_sentences/2.086.json\n",
      "data/corpus_sentences/2.087.json\n",
      "data/corpus_sentences/2.088.json\n",
      "data/corpus_sentences/2.089.json\n",
      "data/corpus_sentences/2.090.json\n",
      "data/corpus_sentences/2.091.json\n",
      "data/corpus_sentences/2.092.json\n",
      "data/corpus_sentences/2.093.json\n",
      "data/corpus_sentences/2.094.json\n",
      "data/corpus_sentences/2.095.json\n",
      "data/corpus_sentences/2.096.json\n",
      "data/corpus_sentences/2.097.json\n",
      "data/corpus_sentences/2.098.json\n",
      "data/corpus_sentences/2.099.json\n",
      "data/corpus_sentences/2.100.json\n",
      "data/corpus_sentences/2.101.json\n",
      "data/corpus_sentences/2.102.json\n",
      "data/corpus_sentences/2.103.json\n",
      "data/corpus_sentences/2.104.json\n",
      "data/corpus_sentences/2.105.json\n",
      "data/corpus_sentences/2.106.json\n",
      "data/corpus_sentences/2.107.json\n",
      "data/corpus_sentences/2.108.json\n",
      "data/corpus_sentences/2.109.json\n",
      "CPU times: user 37min, sys: 10min 42s, total: 47min 42s\n",
      "Wall time: 52min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create or open the database we're going to be writing to.\n",
    "db = xapian.WritableDatabase(DBPATH, xapian.DB_CREATE_OR_OPEN)\n",
    "\n",
    "# Set up a TermGenerator that we'll use in indexing.\n",
    "termgenerator = xapian.TermGenerator()\n",
    "termgenerator.set_stemmer(xapian.Stem(\"en\"))\n",
    "\n",
    "for path in sorted(glob('data/corpus_sentences/*.json')):\n",
    "    if re.match(\".*\\/3.\\d+\\.json\", path):\n",
    "        continue\n",
    "    print(path)\n",
    "    \n",
    "    shard = \"\"\n",
    "    m = re.match(\".*\\/(\\d+).json\", path)\n",
    "    if m: shard = m[1]\n",
    "    #print(shard)\n",
    "\n",
    "    df = read_ner_data(path)\n",
    "    df.reset_index(inplace=True)\n",
    "    df['id'] = df.page_id + '_' + df.sentence.astype(str)\n",
    "    df.set_index('id', inplace=True)\n",
    "    for doc_id, data in df.iterrows():\n",
    "        try:\n",
    "            page_id, sent_id, text, _, keywords = data\n",
    "            \n",
    "            # We make a document and tell the term generator to use this.\n",
    "            doc = xapian.Document()\n",
    "            termgenerator.set_document(doc)\n",
    "\n",
    "            # Index fields without prefixes for general search.\n",
    "            termgenerator.index_text(text)\n",
    "            termgenerator.increase_termpos()\n",
    "\n",
    "            # We use the identifier to ensure each object ends up in the\n",
    "            # database only once no matter how many times we run the\n",
    "            # indexer.\n",
    "            idterm = u\"Q\" + doc_id\n",
    "            doc.add_boolean_term(idterm)\n",
    "            doc.add_boolean_term(u\"S\" + re.sub('-', '_', page_id.lower()))\n",
    "            \n",
    "            for item in keywords:\n",
    "                doc.add_term(u\"K\" + item.lower())\n",
    "\n",
    "            # save additional data\n",
    "            data = dict(\n",
    "                shard = shard,\n",
    "                page_id = page_id,\n",
    "                sentence_id = sent_id,\n",
    "                text = text,\n",
    "                keywords = list(keywords),\n",
    "            )\n",
    "            doc.set_data(json.dumps(data))\n",
    "            \n",
    "            db.replace_document(idterm, doc)\n",
    "        except Exception as e:\n",
    "            print(doc_id, text, e)\n",
    "\n",
    "db.commit()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UUID = 74b38802-8662-47ba-b908-18f8945fc71e\r\n",
      "number of documents = 5342243\r\n",
      "average document length = 41.2869\r\n",
      "document length lower bound = 1\r\n",
      "document length upper bound = 1110\r\n",
      "highest document id ever used = 5342243\r\n",
      "has positional information = true\r\n",
      "revision = 1146\r\n",
      "currently open for writing = false\r\n"
     ]
    }
   ],
   "source": [
    "!xapian-delve $DBPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for record #112:\r\n",
      "{\"shard\": \"001\", \"page_id\": \"17_Again_-LRB-film-RRB-\", \"sentence_id\": 0, \"text\": \"17 Again is a 2009 American comedy film directed by Burr Steers .\", \"keywords\": [\"2009\", \"2009_American_comedy_film\", \"Burr_Steers\", \"17\", \"Steers\", \"American\", \"film\"]}\r\n",
      "Term List for record #112: 17 2009 K17 K2009 K2009_american_comedy_film Kamerican Kburr_steers Kfilm Ksteers Q17_Again_-LRB-film-RRB-_0 S17_again__lrb_film_rrb_ Za Zagain Zamerican Zburr Zby Zcomedi Zdirect Zfilm Zis Zsteer a again american burr by comedy directed film is steers\r\n"
     ]
    }
   ],
   "source": [
    "!xapian-delve -r 112 -d $DBPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
