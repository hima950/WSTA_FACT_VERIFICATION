{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'en_core_web_sm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f1fd3a8eecdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'en_core_web_sm'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import collections\n",
    "import os \n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"search_results_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_id</th>\n",
       "      <th>found_doc</th>\n",
       "      <th>rank</th>\n",
       "      <th>percentage</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Chris_Hemsworth</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>35.874716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Hemsworth_-LRB-surname-RRB-</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>26.919077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Kim_Hyde</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>22.099664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Liam_Hemsworth</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "      <td>21.552361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Vanessa_Zachos</td>\n",
       "      <td>5</td>\n",
       "      <td>37</td>\n",
       "      <td>21.329802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   claim_id                    found_doc  rank  percentage     weight\n",
       "0         3              Chris_Hemsworth     1          63  35.874716\n",
       "1         3  Hemsworth_-LRB-surname-RRB-     2          47  26.919077\n",
       "2         3                     Kim_Hyde     3          39  22.099664\n",
       "3         3               Liam_Hemsworth     4          38  21.552361\n",
       "4         3               Vanessa_Zachos     5          37  21.329802"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_mask = df[\"rank\"] <3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11c2a39e8>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(y=df[r_mask][\"weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Firefox OS was designed to provide a complete, community-based alternative operating system, for running web applications directly or those installed from an application marketplace .'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_mask = df[\"percentage\"] > 50\n",
    "df_1 = df[percent_mask]\n",
    "weight_mask = df_1[\"weight\"] > 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_1[weight_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(183460, 7)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"test-unlabelled.json\"\n",
    "with open(file) as train_file:\n",
    "    test = json.load(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[\"claim\"] = df_1[\"claim_id\"].apply(lambda x: test[str(x)][\"claim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_id</th>\n",
       "      <th>found_doc</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>rank</th>\n",
       "      <th>percentage</th>\n",
       "      <th>weight</th>\n",
       "      <th>text</th>\n",
       "      <th>claim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Firefox_OS</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>18.63</td>\n",
       "      <td>Firefox OS was designed to provide a complete, community-based alternative operating system, for...</td>\n",
       "      <td>Firefox is an application.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>XUL</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>15.54</td>\n",
       "      <td>Waterfox, another fork of Firefox for Windows, macOS, and Linux is planning continued developmen...</td>\n",
       "      <td>Firefox is an application.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>XUL</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>10.14</td>\n",
       "      <td>Pale Moon, a fork of Firefox for Windows and Linux, will continue to support XUL indefinitely .</td>\n",
       "      <td>Firefox is an application.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>Firefox_3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>16.12</td>\n",
       "      <td>Mozilla Firefox 3.5 is a version of the Firefox web browser released in June 2009, adding a vari...</td>\n",
       "      <td>Firefox is an application.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6</td>\n",
       "      <td>Firefox_Portable</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>16.71</td>\n",
       "      <td>Mozilla Firefox, Portable Edition (formerly known as Portable Firefox and commonly known as Fire...</td>\n",
       "      <td>Firefox is an application.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    claim_id         found_doc  sent_id  rank  percentage  weight  \\\n",
       "0          6        Firefox_OS        5     1          80   18.63   \n",
       "5          6               XUL        9     1          80   15.54   \n",
       "6          6               XUL        8     2          52   10.14   \n",
       "10         6       Firefox_3.5        0     1          60   16.12   \n",
       "15         6  Firefox_Portable        0     1          60   16.71   \n",
       "\n",
       "                                                                                                   text  \\\n",
       "0   Firefox OS was designed to provide a complete, community-based alternative operating system, for...   \n",
       "5   Waterfox, another fork of Firefox for Windows, macOS, and Linux is planning continued developmen...   \n",
       "6       Pale Moon, a fork of Firefox for Windows and Linux, will continue to support XUL indefinitely .   \n",
       "10  Mozilla Firefox 3.5 is a version of the Firefox web browser released in June 2009, adding a vari...   \n",
       "15  Mozilla Firefox, Portable Edition (formerly known as Portable Firefox and commonly known as Fire...   \n",
       "\n",
       "                         claim  \n",
       "0   Firefox is an application.  \n",
       "5   Firefox is an application.  \n",
       "6   Firefox is an application.  \n",
       "10  Firefox is an application.  \n",
       "15  Firefox is an application.  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.rename(columns={\"text\":\"evidence\",\"claim_id\":\"ID\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.to_csv(\"test_set_filtered_.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(183460, 8)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_id</th>\n",
       "      <th>found_doc</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>rank</th>\n",
       "      <th>percentage</th>\n",
       "      <th>weight</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Firefox_OS</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>18.63</td>\n",
       "      <td>Firefox OS was designed to provide a complete, community-based alternative operating system, for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>Firefox_OS</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>11.47</td>\n",
       "      <td>It is based on the rendering engine of the Firefox web browser, Gecko, and on the Linux kernel .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>Firefox_OS</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>9.42</td>\n",
       "      <td>Firefox OS (project name : Boot to Gecko, also known as B2G) is a discontinued open-source opera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>Firefox_OS</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "      <td>9.19</td>\n",
       "      <td>In December 2015 Mozilla announced it would stop development of new Firefox OS smartphones, and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Firefox_OS</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>8.35</td>\n",
       "      <td>As such, Mozilla with Firefox OS competed with commercially developed operating systems such as ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   claim_id   found_doc  sent_id  rank  percentage  weight  \\\n",
       "0         6  Firefox_OS        5     1          80   18.63   \n",
       "1         6  Firefox_OS        1     2          49   11.47   \n",
       "2         6  Firefox_OS        0     3          40    9.42   \n",
       "3         6  Firefox_OS        8     4          39    9.19   \n",
       "4         6  Firefox_OS        7     5          35    8.35   \n",
       "\n",
       "                                                                                                  text  \n",
       "0  Firefox OS was designed to provide a complete, community-based alternative operating system, for...  \n",
       "1     It is based on the rendering engine of the Firefox web browser, Gecko, and on the Linux kernel .  \n",
       "2  Firefox OS (project name : Boot to Gecko, also known as B2G) is a discontinued open-source opera...  \n",
       "3  In December 2015 Mozilla announced it would stop development of new Firefox OS smartphones, and ...  \n",
       "4  As such, Mozilla with Firefox OS competed with commercially developed operating systems such as ...  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1428209, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14997/14997 [04:52<00:00, 52.15it/s]\n"
     ]
    }
   ],
   "source": [
    "all_dfs = []\n",
    "for key in tqdm(keys):\n",
    "    all_dfs.append(df[df.claim_id==int(key)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.concat(all_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"claim\"] = test_df[\"claim_id\"].apply(lambda x: test[str(x)][\"claim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"test_docs_top100.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = test_df[\"rank\"] < int(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104979, 6)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"wiki.json\"\n",
    "with open(file) as train_file:\n",
    "    wiki = json.load(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_train = test_df.to_json(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_train = json.loads(dict_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('10016172',\n",
       " {'claim': 'SummerSlam was promoted by WWE in 2015.',\n",
       "  'claim_id': 137129,\n",
       "  'found_doc': 'NXT_TakeOver-COLON-_Brooklyn',\n",
       "  'percentage': 54,\n",
       "  'rank': 3,\n",
       "  'weight': 28.2813489992})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dict_train.items())[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_text_from_line(line):\n",
    "    # use this function to extarct the text from a line in data-file\n",
    "    #try:\n",
    "    line_list =line.split(\" \")\n",
    "    page_id = line_list.pop(0)\n",
    "    page_name = \" \".join(page_id.split(\"_\"))\n",
    "    sentence_id = 12345\n",
    "    if not line_list:\n",
    "        return \" \"\n",
    "    if line_list[0].isnumeric():\n",
    "        sentence_id = line_list.pop(0)\n",
    "    line = \" \".join(line_list)\n",
    "    return (sentence_id, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def strip_accents(text):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFKD', text) if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "total_len = 0\n",
    "for key, val in dict_train.items():\n",
    "    evidence_text_lst = []\n",
    "    item = val[\"found_doc\"]\n",
    "    item = strip_accents(item)\n",
    "        #if item in wiki.keys():\n",
    "    \n",
    "    for line in wiki[item]:\n",
    "        sen_id, text = extract_final_text_from_line(line)\n",
    "        evidence_text_lst.extend(wiki[item])\n",
    "    dict_train[key][\"doc_evidence\"] = evidence_text_lst\n",
    "    total_len += len(evidence_text_lst)\n",
    "        \n",
    "print(\"total evidence:\",total_len)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dff = pd.read_json(json.dumps(dict_train),orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 2, 3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.literal_eval(\"[12,2,3]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_func(df):\n",
    "    ls = []\n",
    "    for i in ast.literal_eval(df[\"doc_evidence\"]):\n",
    "        sen_i, line = extract_final_text_from_line(i)\n",
    "        dff[\"evidence\"] = line\n",
    "        dff[\"sentence_id\"] =sen_i\n",
    "        dff[\"claim\"] = df.claim\n",
    "        dff[\"document\"] = df.found_doc\n",
    "        dff[\"rank\"] = df['rank']\n",
    "        dff[\"claim_id\"] = df.claim_id\n",
    "        ls.append(dff)\n",
    "    return pd.concat(ls)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = pd.read_csv(\"test_top10.csv\")a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff[\"doc_evidence\"] = dff[\"doc_evidence\"].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff_json = dff.to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104979/104979 [01:52<00:00, 933.12it/s] \n"
     ]
    }
   ],
   "source": [
    "cols = []\n",
    "for key,item in tqdm(dff_json.items()):\n",
    "    for i in item[\"doc_evidence\"]:\n",
    "        d = {}\n",
    "        sen_i, line = extract_final_text_from_line(i)\n",
    "        d[\"evidence\"] = line\n",
    "        d[\"sentence_id\"] =sen_i\n",
    "        d[\"claim\"] = item[\"claim\"]\n",
    "        d[\"document\"] = item[\"found_doc\"]\n",
    "        d[\"rank\"] = item['rank']\n",
    "        d[\"claim_id\"] = item[\"claim_id\"]\n",
    "        cols.append(d)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35324033, 6)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(793308, 6)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"test_search_top_10.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTask():\n",
    "    def __init__(self):\n",
    "        self.nlp = en_core_web_sm.load()\n",
    "        \n",
    "    def get_POS_tags_list(self, text):\n",
    "        docs = self.nlp(text)\n",
    "        pos_list = []\n",
    "        for word in docs:\n",
    "            pos_list.append((word.text,word.pos_))\n",
    "\n",
    "        return pos_list\n",
    "    def get_enitities_list(self, text):\n",
    "        docs = self.nlp(text)\n",
    "        entities_dict = {word: word.label_ for word in docs.ents}\n",
    "        return entities_dict\n",
    "    \n",
    "    def tokenise_and_lower(self, text):\n",
    "        docs = self.nlp(text)\n",
    "        tokens = []\n",
    "        for word in docs:\n",
    "            tokens.append(str(word).lower())\n",
    "        return tokens\n",
    "                        \n",
    "    \n",
    "    def grab_evidence_text(self, ID):\n",
    "        ID = ID.strip()\n",
    "        !grep -r ID /Users/hima95/Downloads/wiki-pages-text/ >> search.txt\n",
    "        fp = open(\"./search.txt\",'r')\n",
    "        text = fp.readlines()\n",
    "        fp.close()\n",
    "        os.remove(\"./search.txt\")\n",
    "        return text\n",
    "    \n",
    "    def extract_final_text_from_line(self, line):\n",
    "        # use this function to extarct the text from a line in data-file\n",
    "        #try:\n",
    "        line_list =line.split(\" \")\n",
    "        page_id = line_list.pop(0)\n",
    "        page_name = \" \".join(page_id.split(\"_\"))\n",
    "        sentence_id = 12345\n",
    "        if not line_list:\n",
    "            return \" \"\n",
    "        if line_list[0].isnumeric():\n",
    "            sentence_id = line_list.pop(0)\n",
    "        return \" \".join(line_list)\n",
    "        #except:\n",
    "        #    print(\"Something went wrong\")\n",
    "\n",
    "    \n",
    "    def longest_common_sequence(self, t1, t2):\n",
    "        tkns1, tkns2 = t1.split(), t2.split()\n",
    "        counter = collections.defaultdict(dict)\n",
    "        for i in range(-1, len(tkns1)):\n",
    "            for j in range(-1, len(tkns2)):\n",
    "                if i == -1 or j == -1:\n",
    "                    counter[i][j] = 0\n",
    "                else:\n",
    "                    if tkns1[i] == tkns2[j]:\n",
    "                        counter[i][j] = counter[i - 1][j - 1] + 1\n",
    "                    else:\n",
    "                        counter[i][j] = max(counter[i - 1][j], counter[i][j - 1])\n",
    "        return counter[len(tkns1) - 1][len(tkns2) - 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosCountsTask(BaseTask):\n",
    "    def __init__(self,df):\n",
    "        self.df = df\n",
    "        self.desired_pos = [\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\"]\n",
    "        self.grammar_pos = [\"INTJ\", \"CCONJ\", \"AUX\", \"PUNCT\",\"PART\",\"SCONJ\",\"DET\",\"SYM\", \"NUM\"]\n",
    "        super().__init__()\n",
    "        \n",
    "    def make_pos_counts(self, text, pos_tag):\n",
    "        docs = self.nlp(text)\n",
    "        counts = Counter()\n",
    "        for word in docs:\n",
    "            counts[word.pos_] += 1\n",
    "        # combine all counts of other grammar words than the desired list\n",
    "        counts2 = Counter()\n",
    "        for pos in counts.keys():\n",
    "            if pos in self.grammar_pos:\n",
    "                counts2[\"other_pos_counts\"] += counts[pos]\n",
    "        \n",
    "        if pos_tag in counts2.keys():\n",
    "            return counts2[pos_tag]\n",
    "        elif pos_tag in counts.keys():\n",
    "            return counts[pos_tag]\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def make_counts_fields(self):\n",
    "        df = self.df\n",
    "        for pos in self.desired_pos:\n",
    "            field_name = spacy.explain(pos) + \"_counts\"\n",
    "            df[field_name] = df[\"claim\"].apply(lambda x: self.make_pos_counts(x,pos))\n",
    "            \n",
    "        df[\"other_pos_counts\"] = self.df[\"claim\"].apply(lambda x: \n",
    "                                                           self.make_pos_counts(x,\"other_pos_counts\"))\n",
    "        return df\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordsAndEntityTask(BaseTask):\n",
    "    def __init__(self,df):\n",
    "        self.df = df\n",
    "        self.key_pos = [\"NOUN\", \"PROPN\", \"SYM\", \"NUM\",\"ADJ\"]\n",
    "        super().__init__()\n",
    "        \n",
    "    def extract_keyword_list(self, claim):\n",
    "        # Pick all the Entities and the key POS in the list \n",
    "\n",
    "        ents = self.get_enitities_list(claim)\n",
    "\n",
    "        pos = self.get_POS_tags_list(claim)\n",
    "\n",
    "        pos_filtered = [(key,value) for key,value in pos if value in self.key_pos]\n",
    "\n",
    "        ents_list = list(ents.keys())\n",
    "        ents_list = [str(item) for item in ents_list]\n",
    "        # pos list of the entities\n",
    "        pos_of_ents = self.get_POS_tags_list(\" \".join(ents_list))\n",
    "        pos_of_ents = [key for key,value in pos_of_ents]\n",
    "        # list of all filtered pos\n",
    "        pos_list = [key for key,value in pos_filtered] \n",
    "        # keywords not in entities\n",
    "        other_keywords = [word for word in pos_list if word not in pos_of_ents]\n",
    "\n",
    "        final_key_words = ents_list + other_keywords\n",
    "        return final_key_words\n",
    "    \n",
    "    def caliculate_keywords_len(self):\n",
    "        df = self.df\n",
    "        \n",
    "        df[\"keyword_count\"] = df[\"claim\"].apply(lambda x: len(self.extract_keyword_list(x)))\n",
    "        return df \n",
    "    \n",
    "    def keywords_similarity(self, claim, candidate_sent):\n",
    "        # caliculate similarity b/w claim and candidate sentence using only thier key words\n",
    "        clm = self.nlp(\" \".join(self.extract_keyword_list(claim)))\n",
    "        evdc = self.nlp(\" \".join(self.extract_keyword_list(candidate_sent)))\n",
    "        return clm.similarity(evdc)\n",
    "        \n",
    "    def common_keywords_count(self, claim, candidate_sent):\n",
    "        clm = [item.lower().strip() for item in self.extract_keyword_list(claim)]\n",
    "        evdc = [item.lower().strip() for item in self.extract_keyword_list(candidate_sent)]\n",
    "        return len(list(set(evdc).intersection(clm)))\n",
    "    \n",
    "    def caliculate_jacards_similarity(self, claim,evidence):\n",
    "        # caliculate the jacards similarity betwen keywords of the two sentences\n",
    "        claim = self.extract_keyword_list(claim)\n",
    "        evidence = self.extract_keyword_list(evidence)\n",
    "        intersection = set(evidence).intersection(set(claim))\n",
    "        union = set(claim).union(set(evidence))\n",
    "        return len(intersection)/len(union)\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os \n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'testset_query_results.json'\n",
    "with open(file) as train_file:\n",
    "    test = json.load(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"search_results_v2_ner_index.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16544416, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_id</th>\n",
       "      <th>found_doc</th>\n",
       "      <th>rank</th>\n",
       "      <th>percentage</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Chris_Hemsworth</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>35.874716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Hemsworth_-LRB-surname-RRB-</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>26.919077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Kim_Hyde</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>22.099664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Liam_Hemsworth</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "      <td>21.552361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Vanessa_Zachos</td>\n",
       "      <td>5</td>\n",
       "      <td>37</td>\n",
       "      <td>21.329802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   claim_id                    found_doc  rank  percentage     weight\n",
       "0         3              Chris_Hemsworth     1          63  35.874716\n",
       "1         3  Hemsworth_-LRB-surname-RRB-     2          47  26.919077\n",
       "2         3                     Kim_Hyde     3          39  22.099664\n",
       "3         3               Liam_Hemsworth     4          38  21.552361\n",
       "4         3               Vanessa_Zachos     5          37  21.329802"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_2 = df['rank'].apply(lambda x: True if x in [1,2] else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"test-unlabelled.json\"\n",
    "with open(file) as train_file:\n",
    "    test = json.load(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_test_results(item):\n",
    "    if str(item) in ids:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df[top_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = df_1.claim_id.apply(filter_test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_1[test_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29994, 5)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.to_csv(\"test_search_top2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the evidence text first add the page name if not present in the evidence text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"devset_query_results.json\"\n",
    "with open(file) as train_file:\n",
    "    dev = json.load(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making data_set for attentions model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5001/5001 [00:00<00:00, 86758.24it/s]\n"
     ]
    }
   ],
   "source": [
    "k = BaseTask()\n",
    "final_dict = {}\n",
    "for key,val in tqdm(dev.items()):\n",
    "    #print(key)\n",
    "    if val[\"evidence\"]:\n",
    "        evidence = \"\"\n",
    "        corr = val[\"gold_evidence\"]\n",
    "        for i in corr:\n",
    "            evidence = evidence + \" \" + k.extract_final_text_from_line(i)\n",
    "        final_dict[key] = {}\n",
    "        final_dict[key][\"label\"] = val[\"label\"]\n",
    "        final_dict[key][\"claim\"] = val[\"claim\"]\n",
    "        final_dict[key][\"evidence\"] = evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('devset_NN_.json',\"w\") as train_file:\n",
    "    json.dump(final_dict, train_file,indent=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5001/5001 [1:16:02<00:00,  1.10it/s]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "k = BaseTask()\n",
    "for key,val in tqdm(dev.items()):\n",
    "    #print(key)\n",
    "    if val[\"evidence\"]:\n",
    "        corr = val[\"gold_evidence\"]\n",
    "        wrong = val[\"wrong_evidence\"]\n",
    "        corr = [k.extract_final_text_from_line(i) for i in corr]\n",
    "        wrong = [k.extract_final_text_from_line(i) for i in wrong]\n",
    "        wrong_sample = random.sample(wrong,len(corr))\n",
    "        dev[key][\"wrong_evidence\"] = wrong_sample\n",
    "        dev[key][\"gold_evidence\"] = corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('devset_query_results1.json',\"w\") as train_file:\n",
    "    json.dump(dev, train_file,indent=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating train csv for MLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"MLU_train_sample.json\"\n",
    "with open(file) as train_file:\n",
    "    dev = json.load(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24000/24000 [20:45<00:00, 20.98it/s] \n"
     ]
    }
   ],
   "source": [
    "k = BaseTask()\n",
    "cols =[]\n",
    "for key,val in tqdm(dev.items()):\n",
    "    claim = val[\"claim\"]\n",
    "    evd = val[\"evidence_text\"]  \n",
    "    evd_p = [k.extract_final_text_from_line(i) for i in val[\"evidence_text\"]]\n",
    "    evd_p = \" \".join(evd_p)\n",
    "    cols_dict = {\"claim\":claim,\"evidence\":evd_p,\"ID\":key,\"label\":val[\"label\"]}\n",
    "    cols.append(cols_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"train_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 4)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sup = df[df[\"label\"]==\"SUPPORTS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80035, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref = df[df[\"label\"]==\"REFUTES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29775, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ref.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_not = df[df[\"label\"]==\"NOT ENOUGH INFO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boostratp sample\n",
    "# scikit-learn bootstrap\n",
    "from sklearn.utils import resample\n",
    "# prepare bootstrap sample\n",
    "df_sup = resample(df_sup, replace=False, n_samples=12000, random_state=123)\n",
    "df_ref = resample(df_ref, replace=False, n_samples=12000, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.concat([df_sup,df_ref],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_json(\"MLU_train_sample.json\",orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"bootstrap_train_MLU.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'testset_query_results.json'\n",
    "with open(file) as train_file:\n",
    "    dev = json.load(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14997/14997 [1:36:19<00:00,  2.14it/s]  \n"
     ]
    }
   ],
   "source": [
    "k = BaseTask()\n",
    "cols =[]\n",
    "for key,val in tqdm(dev.items()):\n",
    "    claim = val[\"claim\"]\n",
    "    evd = val[\"evidence\"]\n",
    "    evd_p = [k.extract_final_text_from_line(i) for i in val[\"evidence\"]]\n",
    "    for item,item_p in zip(evd,evd_p):\n",
    "        line_list =item.split(\" \")\n",
    "        page_id = line_list.pop(0)\n",
    "        sentence_id = 123\n",
    "        if line_list[0].isnumeric():\n",
    "            sentence_id = line_list.pop(0)\n",
    "        cols_dict = {\"claim\":claim,\"evidence\":item_p,\"ID\":key,\"evidence_id\":[page_id,sentence_id]}\n",
    "        cols.append(cols_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(236023, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"bootstrap_train_MLU.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"test_evidencePairs_top2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(236023, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(236023, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandarallel import pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New pandarallel memory created - Size: 2000 MB\n",
      "Pandarallel will run on 2 workers\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize(nb_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = BaseTask()\n",
    "key = KeywordsAndEntityTask(df=None) \n",
    "def make_jacards_features(tup):\n",
    "    claim,evidence = tup\n",
    "    claim = k.extract_final_text_from_line(claim)\n",
    "    evidence = k.extract_final_text_from_line(evidence)\n",
    "    return key.caliculate_jacards_similarity(claim,evidence)\n",
    "def make_keywords_similarity_features(tup):\n",
    "    claim,evidence = tup\n",
    "    claim = k.extract_final_text_from_line(claim)\n",
    "    evidence = k.extract_final_text_from_line(evidence)\n",
    "    return key.keywords_similarity(claim,evidence)\n",
    "def make_common_key_words_count(tup):\n",
    "    claim,evidence = tup\n",
    "    claim = k.extract_final_text_from_line(claim)\n",
    "    evidence = k.extract_final_text_from_line(evidence)\n",
    "    return key.common_keywords_count(claim,evidence)\n",
    "def make_lcs(tup):\n",
    "    claim,evidence = tup\n",
    "    claim = k.extract_final_text_from_line(claim)\n",
    "    evidence = k.extract_final_text_from_line(evidence)\n",
    "    return k.longest_common_sequence(claim,evidence)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "df[\"Jacards_Similarity\"]  = df[['claim', 'evidence']].apply(make_jacards_features,axis=1)\n",
    "df[\"Keywords_Similarity\"]  = df[['claim', 'evidence']].apply(make_keywords_similarity_features,axis=1)\n",
    "df[\"common_keywords\"]  = df[['claim', 'evidence']].apply(make_common_key_words_count,axis=1)\n",
    "df[\"LCS\"]  = df[['claim', 'evidence']].apply(make_lcs,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, broadcast, raw, reduce, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   6485\u001b[0m                          \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6486\u001b[0m                          kwds=kwds)\n\u001b[0;32m-> 6487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m                                           \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                                           \u001b[0mdummy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                                           labels=labels)\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.reduce\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.Reducer.get_result\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-898949fc1339>\u001b[0m in \u001b[0;36mmake_keywords_similarity_features\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mclaim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_final_text_from_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclaim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mevidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_final_text_from_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevidence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeywords_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclaim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mevidence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_common_key_words_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mclaim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mevidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-e5973a35faf8>\u001b[0m in \u001b[0;36mkeywords_similarity\u001b[0;34m(self, claim, candidate_sent)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# caliculate similarity b/w claim and candidate sentence using only thier key words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mclm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_keyword_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclaim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mevdc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_keyword_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mclm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevdc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__call__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.parse_batch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.get_batch_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcontinue_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(seqs_in, drop)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseqs_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         X, bp_layer = layer.begin_update(layer.ops.flatten(seqs_in, pad=pad),\n\u001b[0;32m--> 280\u001b[0;31m                                          drop=drop)\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcontinue_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/resnet.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbp_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mresidual_bwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcontinue_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/layernorm.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprop_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_moments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/maxout.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X__bi, drop)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnO\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mdrop\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0moutput__boc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX__bi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0moutput__boc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnO\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0moutput__boc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput__boc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput__boc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df[\"Jacards_Similarity\"]  = df[['claim', 'evidence']].apply(make_jacards_features,axis=1)\n",
    "df[\"Keywords_Similarity\"]  = df[['claim', 'evidence']].apply(make_keywords_similarity_features,axis=1)\n",
    "df[\"common_keywords\"]  = df[['claim', 'evidence']].apply(make_common_key_words_count,axis=1)\n",
    "df[\"LCS\"]  = df[['claim', 'evidence']].apply(make_lcs,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"bootstrap_train_MLU.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25683</td>\n",
       "      <td>David Dhawan works on Hindi films.</td>\n",
       "      <td>David Dhawan -LRB- born Rajinder Dhawan on 16 August 1955 -RRB- is an Indian film director who w...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5307</td>\n",
       "      <td>Pink Floyd's principal songwriter was Syd Barrett.</td>\n",
       "      <td>Roger Keith `` Syd '' Barrett -LRB- 6 January 1946 -- 7 July 2006 -RRB- was a British musician ,...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2483</td>\n",
       "      <td>Neil Armstrong was in the first in-flight space emergency.</td>\n",
       "      <td>Neil Armstrong This mission was aborted after Armstrong used some of his reentry control fuel to...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40881</td>\n",
       "      <td>Emilio Estevez is a human.</td>\n",
       "      <td>Emilio Estevez He is also known for Repo Man , The Mighty Ducks and its sequels , Stakeout , Max...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>227607</td>\n",
       "      <td>Born This Way drew criticism from Asian communities.</td>\n",
       "      <td>Born This Way -LRB-song-RRB- The lyrics discuss the self-empowerment of minorities including the...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                                                       claim  \\\n",
       "0   25683                          David Dhawan works on Hindi films.   \n",
       "1    5307          Pink Floyd's principal songwriter was Syd Barrett.   \n",
       "2    2483  Neil Armstrong was in the first in-flight space emergency.   \n",
       "3   40881                                  Emilio Estevez is a human.   \n",
       "4  227607        Born This Way drew criticism from Asian communities.   \n",
       "\n",
       "                                                                                              evidence  \\\n",
       "0  David Dhawan -LRB- born Rajinder Dhawan on 16 August 1955 -RRB- is an Indian film director who w...   \n",
       "1  Roger Keith `` Syd '' Barrett -LRB- 6 January 1946 -- 7 July 2006 -RRB- was a British musician ,...   \n",
       "2  Neil Armstrong This mission was aborted after Armstrong used some of his reentry control fuel to...   \n",
       "3  Emilio Estevez He is also known for Repo Man , The Mighty Ducks and its sequels , Stakeout , Max...   \n",
       "4  Born This Way -LRB-song-RRB- The lyrics discuss the self-empowerment of minorities including the...   \n",
       "\n",
       "      label  \n",
       "0  SUPPORTS  \n",
       "1  SUPPORTS  \n",
       "2  SUPPORTS  \n",
       "3  SUPPORTS  \n",
       "4  SUPPORTS  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.extract_final_text_from_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the train json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'train_final.json'\n",
    "with open(file) as train_file:\n",
    "    train = json.load(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_d = {key:val for key,val in train.items() if val[\"label\"] == \"SUPPORTS\"}\n",
    "ref_d = {key:val for key,val in train.items() if val[\"label\"] == \"REFUTES\"}\n",
    "no_if = {key:val for key,val in train.items() if val[\"label\"] == \"NOT ENOUGH INFO\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_keys = random.sample(list(sup_d.keys()),k=7500)\n",
    "ref_keys = random.sample(list(ref_d.keys()),k=7500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_d = {key:train[key] for key in sup_keys+ref_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:00<00:00, 60199.39it/s]\n"
     ]
    }
   ],
   "source": [
    "k = BaseTask()\n",
    "final_dict = {}\n",
    "for key,val in tqdm(sampled_d.items()):\n",
    "    #print(key)\n",
    "    if val[\"evidence\"]:\n",
    "        evidence = \"\"\n",
    "        corr = val[\"evidence_text\"]\n",
    "        for i in corr:\n",
    "            evidence = evidence + \" \" + k.extract_final_text_from_line(i)\n",
    "        final_dict[key] = {}\n",
    "        final_dict[key][\"label\"] = val[\"label\"]\n",
    "        final_dict[key][\"claim\"] = val[\"claim\"]\n",
    "        final_dict[key][\"evidence\"] = evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('_train_NN_15k_sample.json',\"w\") as train_file:\n",
    "    json.dump(final_dict, train_file,indent=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Train data and add support len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def strip_accents(text):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFKD', text) if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for key, val in dev_train.items():\n",
    "    corr = []\n",
    "    evd = dev_train[key][\"evidence\"]\n",
    "    if evd:\n",
    "        for item in evd:\n",
    "            page = item[0]\n",
    "            evd_str = \" \".join(list(map(str,item)))\n",
    "            doc = wiki.get(page)\n",
    "            if doc:\n",
    "                for line in doc:\n",
    "                    if evd_str in line:\n",
    "                        corr.append(line)\n",
    "        dev_train[key][\"evidence\"] = corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('devset_1.json',\"w\") as train_file:\n",
    "    json.dump(dev_train, train_file,indent=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'devset_1.json'\n",
    "with open(file) as train_file:\n",
    "    dev_train = json.load(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_d = {}\n",
    "for key,val in dev_Q.items():\n",
    "    corr = []\n",
    "    wrong = []\n",
    "    claim = dev_train[key][\"claim\"]\n",
    "    for line in val:\n",
    "        line = line.replace(\"\\n\",\"\").strip()\n",
    "        edvc_list = [item.strip() for item in dev_train[key][\"evidence\"]]\n",
    "        if line not in edvc_list:\n",
    "            wrong.append(line)\n",
    "        corr = edvc_list\n",
    "    final_d[key] = dict(gold_evidence = corr,wrong_evidence = wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'devset.json'\n",
    "with open(file) as train_file:\n",
    "    dev_train = json.load(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,val in final_d.items():\n",
    "    dev_train[key][\"gold_evidence\"] = val[\"gold_evidence\"]\n",
    "    dev_train[key][\"wrong_evidence\"] = val[\"wrong_evidence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('devset_query_results1.json',\"w\") as train_file:\n",
    "    json.dump(dev_train, train_file,indent=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_counts = KeywordsAndEntityTask(df=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-5b58175f9029>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mclaim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"claim\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeywords_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclaim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mcomm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon_keywords_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclaim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdev_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"similarity_scores\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-0c7512ba84bc>\u001b[0m in \u001b[0;36mkeywords_similarity\u001b[0;34m(self, claim, candidate_sent)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# caliculate similarity b/w claim and candidate sentence using only thier key words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mclm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_keyword_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclaim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mevdc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_keyword_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mclm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevdc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-0c7512ba84bc>\u001b[0m in \u001b[0;36mextract_keyword_list\u001b[0;34m(self, claim)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Pick all the Entities and the key POS in the list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0ments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enitities_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclaim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_POS_tags_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclaim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f0a06e3b220f>\u001b[0m in \u001b[0;36mget_enitities_list\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpos_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_enitities_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mentities_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mentities_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__call__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.parse_batch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.get_batch_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.precompute_hiddens.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/describe.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, type)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/mem.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[1;32m   2770\u001b[0m     \"\"\"\n\u001b[1;32m   2771\u001b[0m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2772\u001b[0;31m                           initial=initial)\n\u001b[0m\u001b[1;32m   2773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for key,val in dev_train.items():\n",
    "    sim = []\n",
    "    comm = []\n",
    "    claim = dev_train[key][\"claim\"]\n",
    "    for item in val:\n",
    "        sim.append(key_counts.keywords_similarity(claim,item))\n",
    "        comm.append(key_counts.common_keywords_count(claim,item))\n",
    "    dev_train[key][\"similarity_scores\"] = sim\n",
    "    dev_train[key][\"common_keywords\"] = comm\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "new_train_dict = {}\n",
    "for key,value in dict_train.items():\n",
    "    value[\"Support_length\"] = len(value['evidence'])\n",
    "    new_train_dict[key] = value"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('train_2.json',\"w\") as train_file:\n",
    "    json.dump(new_train_dict, train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json('train_2.json', orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sort_index().tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pos_counts = PosCountsTask(df=train_df)\n",
    "df = pos_counts.make_counts_fields()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "key_counts = KeywordsAndEntityTask(df=df)\n",
    "df = key_counts.caliculate_keywords_len()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"initial_features-devset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-0052ddc42c4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkey_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeywordsAndEntityTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "key_counts = KeywordsAndEntityTask(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim = \"Chris Hemsworth appeared in A Perfect Getaway.\"\n",
    "evidence = \"Hemsworth has also appeared in the science fiction action film Star Trek -LRB- 2009 -RRB- , the thriller adventure A Perfect Getaway -LRB- 2009 -RRB- , the horror comedy The Cabin in the Woods -LRB- 2012 -RRB- , the dark-fantasy action film Snow White and the Huntsman -LRB- 2012 -RRB- , the war film Red Dawn -LRB- 2012 -RRB- , and the biographical sports drama film Rush -LRB- 2013 -RRB- .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5100020158785087"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_counts.keywords_similarity(claim,evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_counts.common_keywords_count(claim,evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chris Hemsworth', 'Perfect', 'Getaway']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_counts.extract_keyword_list(claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/decomposable-attention-elmo-2018.02.19.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f3796018f3e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m predictor.predict(\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mhypothesis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Two women are sitting on a blanket near some rocks talking about politics.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mpremise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Two women are wandering along the shore drinking iced tea.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictor' is not defined"
     ]
    }
   ],
   "source": [
    "predictor.predict(\n",
    "  hypothesis=\"Two women are sitting on a blanket near some rocks talking about politics.\",\n",
    "  premise=\"Two women are wandering along the shore drinking iced tea.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
